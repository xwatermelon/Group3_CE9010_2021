{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b32860ce",
   "metadata": {
    "id": "b32860ce"
   },
   "source": [
    "# CE9010 Introduction To Data Analysis\n",
    "\n",
    "## Group 3\n",
    "\n",
    "Student Name  |  Matric No  \n",
    ":-:|:-:\n",
    "Say Yueyang, Symus|U1922016K   \n",
    "He Zeqing|U1722721G\n",
    "Kwek Yan Qing|U1740743J   \n",
    "\n",
    "---\n",
    "\n",
    "# Background\n",
    "\n",
    "Haze is one major form of air pollution that Singaporeans face annually. The presence of haze is attributed to the forest fires in Sumatra, Indonesia. Due to the increase in demand of commercial crops, Indonesian farmers have resorted to shifting agriculture, which includes the large-scale slash-and-burn of forest land to produce fertile planting grounds. The resulting air pollution is then spread across the region by the climatic phenomenon El Nino, enveloping Singapore in a blanket of haze and affecting the overall health of Singaporeans. In 2020, Indonesia fires torched approximately 207,000 hectares of forests from January to September. While the area is smaller compared to previous years, the burning resulted in a US$5.2 billion cost towards the Indonesian economy, and the occurrence of toxic smog over the city.\n",
    "\n",
    "**References:**\n",
    "- [Haze Pollution](https://eresources.nlb.gov.sg/infopedia/articles/SIP_2013-08-30_185150.html#:~:text=Forest%20fires%20in%20Sumatra%2C%20Indonesia,of%20the%20haze%20in%20Singapore.&text=Strong%20winds%20during%20the%20southwest,such%20fires%20throughout%20Southeast%20Asia.)\n",
    "- [Commentary: Little smoke this haze season â€“ but fires rage on in Indonesia](https://www.channelnewsasia.com/news/commentary/indonesia-forest-fire-peat-haze-palm-oil-jokowi-omnibus-bill-13533700)\n",
    "\n",
    "# Objective\n",
    "\n",
    "The objective of our study is to predict the possible intensity of future hotspots in South East Asia, including Indonesia.\n",
    "Hopefully, this study will be able to support further research in estimating the possibility and severity of the occurrences of haze in Singapore.\n",
    "\n",
    "Our study will be conducted with the relevant data on forest fires in South East Asia. Our dataset is obtained from the National Aeronautics and Space Administration (NASA)'s Fire Information for Resource Management System (FIRMS). It contains both geographical and technical data extracted from the Visible Infrared Imaging Radiometer Suite (VIIRS) sensor aboard their NOAA-20 weather satellite.\n",
    "\n",
    "The table below describes each data available in our dataset:\n",
    "\n",
    "| Data | Description |\n",
    "| -: | :- |\n",
    "| latitude | Indicates the latitude of fire pixel. |\n",
    "| longitude\t| Indicates the longitude of fire pixel. |\n",
    "| bright_ti4 | Indicates the VIIRS I-4 Channel brightness temperature of the fire pixel. |\n",
    "| scan | Indicates the  Along Scan pixel size. |\n",
    "| track\t| Indicates the Along Track pixel size.  |\n",
    "| acq_date | Indicates the date of the acquired data. |\n",
    "| acq_time | Indicates the time of the acquired data. |\n",
    "| satellite | Indicates if the scan was done by the satellite (boolean values). |\n",
    "| confidence | Indicates the confidence level of the data collected. |\n",
    "| version | Indicates the version and source of data processing. |\n",
    "| bright_ti5 | Indicates the VIIRS I-5 Channel brightness temperature of the fire pixel. |\n",
    "| frp | Indicates the Fire Radiative Power (Detected thermal strength of the fire). |\n",
    "| daynight | Indicates whether if it's daytime fire or nighttime fire. |\n",
    "\n",
    "**References:**\n",
    "- [Fire Information for Resource Management System](https://firms2.modaps.eosdis.nasa.gov/)\n",
    "- [Visible Infrared Imaging Radiometer Suite](https://en.wikipedia.org/wiki/Visible_Infrared_Imaging_Radiometer_Suite)\n",
    "- [Attribute Fields](https://earthdata.nasa.gov/earth-observation-data/near-real-time/firms/v1-vnp14imgt#ed-viirs-375m-attributes)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de5ee4a",
   "metadata": {
    "id": "6de5ee4a"
   },
   "source": [
    "# Table of Contents\n",
    "\n",
    "1. [Setup](#1-|-Setup)\n",
    "2. [Dataset Summaries](#2-|-Dataset-Summaries)\n",
    "3. [Exploratory Data Analysis](#3-|-Exploratory-Data-Analysis)\n",
    "4. [Data Pre-Preprocessing](#4-|-Data-Pre-Processing)\n",
    "5. [Data Analysis](#5-|-Data-Analysis)\n",
    "6. [Results Analysis](#6-|-Results-Analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76f3246",
   "metadata": {
    "id": "d76f3246"
   },
   "source": [
    "# 1 | Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jxwqi5DVYWFG",
   "metadata": {
    "id": "Jxwqi5DVYWFG"
   },
   "source": [
    "## 1.1 | Setup on Anaconda Prompt / Local Computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ffcd39",
   "metadata": {
    "id": "16ffcd39"
   },
   "source": [
    "1. Ensure that the environment.yml file accompanying this notebook is located in the same directory as the notebook. <br>\n",
    "2. Open Anaconda Prompt, and in Anaconda Prompt, navigate to the directory where the notebook was downloaded. <br>\n",
    "3. In Anaconda Prompt, enter the following line of code:\n",
    "> conda env create -f environment.yml\n",
    "\n",
    "4. In Anaconda Prompt, enter the following line of code:\n",
    "> conda info --envs\n",
    "\n",
    "If installation of the environment is successful, you will see the environment 'CE9010_2021_Group3' listed in the list of environments.\n",
    "\n",
    "5. Activate the notebook: \n",
    "> conda activate CE9010_2021_Group3\n",
    "\n",
    "6. Run Jupyter Notebook \n",
    "> jupyter notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xVRVngCKYfg2",
   "metadata": {
    "id": "xVRVngCKYfg2"
   },
   "source": [
    "## 1.2 | Setup on Google Colab\n",
    "Run the following cells to: \n",
    "\n",
    "1.   !pip install all the required packages, and\n",
    "2.   Setup connection to Google Drive as your directory (Optional, for accessing files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tBN61h4B-MUD",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35754,
     "status": "ok",
     "timestamp": 1618148165720,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "tBN61h4B-MUD",
    "outputId": "1f14b162-eb77-4ea4-bbd7-7470c5e2e4e1"
   },
   "outputs": [],
   "source": [
    "# For use on Google Colab\n",
    "import sys\n",
    "!pip install numpy\n",
    "!pip install seaborn\n",
    "!pip install plotly\n",
    "!pip install pandas\n",
    "!pip install geopandas\n",
    "!pip install rtree\n",
    "!pip install pygeos\n",
    "!pip install imageio\n",
    "!pip3 install rtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Jr2J1S7QZSXf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 36083,
     "status": "ok",
     "timestamp": 1618152478642,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "Jr2J1S7QZSXf",
    "outputId": "4d18a6fb-b621-4d9c-9940-9dc45556703d"
   },
   "outputs": [],
   "source": [
    "# Set up Google Drive to be the working directory\n",
    "from google.colab import drive # import drive from Google Colab\n",
    " \n",
    "ROOT = \"/content/drive/\"     # default location for the drive\n",
    "print(ROOT)                 # print content of ROOT (Optional)\n",
    " \n",
    "drive.mount(ROOT)           # we mount the Google Drive at /content/drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w7xQh5n3Zjff",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 803,
     "status": "ok",
     "timestamp": 1618152485192,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "w7xQh5n3Zjff",
    "outputId": "f5629967-cdef-47ae-e283-f67d1c5c5d95"
   },
   "outputs": [],
   "source": [
    "%pwd #check that it is correctly mounted\n",
    "%ls #list all the directories available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lJbCOlnEZkG7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1618152519838,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "lJbCOlnEZkG7",
    "outputId": "bc30f4ee-9e43-4e42-f556-516365e18736"
   },
   "outputs": [],
   "source": [
    "#Set github repo path\n",
    "# import join used to join ROOT path and MY_GOOGLE_DRIVE_PATH\n",
    "from os.path import join \n",
    "# path to your project on Google Drive\n",
    "MY_GOOGLE_DRIVE_PATH = \"/content/drive/MyDrive/rootCE9010/repo\" \n",
    " \n",
    "PROJECT_PATH = join(ROOT, MY_GOOGLE_DRIVE_PATH)\n",
    " \n",
    "# It's good to print out the value if you are not sure \n",
    "print(\"PROJECT_PATH: \", PROJECT_PATH)\n",
    "\n",
    "%cd {PROJECT_PATH}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1BC-JgsZ6VP",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1370,
     "status": "ok",
     "timestamp": 1618152523705,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "b1BC-JgsZ6VP",
    "outputId": "02607707-e651-47fe-9c1f-4af4651399a2"
   },
   "outputs": [],
   "source": [
    "%pwd\n",
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EICbp_b5Y-6v",
   "metadata": {
    "id": "EICbp_b5Y-6v"
   },
   "source": [
    "## 1.3 | Import modules\n",
    "Run this to import all the modules required, regardless of what environment you are running in. Make sure that the imports have no errors before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d2e636",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 828,
     "status": "ok",
     "timestamp": 1618148276324,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "47d2e636",
    "outputId": "4f63f070-6b2d-4d92-8cb2-e8fb396520a1"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import os\n",
    "try:\n",
    "    import pandas as pd\n",
    "    import plotly.express as px\n",
    "    from matplotlib import pyplot as plt\n",
    "    from IPython.display import IFrame,Image\n",
    "    import seaborn as sns\n",
    "    import numpy as np\n",
    "    import imageio\n",
    "    import geopandas as gpd\n",
    "    import rtree\n",
    "    import pygeos\n",
    "    print (\"All modules imported successfully.\")\n",
    "except ImportError:\n",
    "    print (\"One or more modules not imported!\")\n",
    "    print (\"Please check that all dependencies are installed.\")\n",
    "\n",
    "# Directory to store local content for loading of interactive images\n",
    "if not os.path.exists(\"content\"):\n",
    "    os.mkdir(\"content\")\n",
    "# Clean up past images\n",
    "else:\n",
    "    for f in os.listdir('./content'):\n",
    "        os.remove(os.path.join('content', f))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508e32f",
   "metadata": {
    "id": "1508e32f"
   },
   "source": [
    "# 2 | Dataset Summaries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a119dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "executionInfo": {
     "elapsed": 1403,
     "status": "ok",
     "timestamp": 1618148284410,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "97a119dd",
    "outputId": "fcbd975e-1c8f-4aef-8650-58f1560f9bcb"
   },
   "outputs": [],
   "source": [
    "# Data Acquisition\n",
    "# Import data from the Active Fire Dataset, VIIRS 375m / NOAA-20\n",
    "data = pd.read_csv(\"https://firms2.modaps.eosdis.nasa.gov/data/active_fire/noaa-20-viirs-c2/csv/J1_VIIRS_C2_SouthEast_Asia_7d.csv\",sep=',')\n",
    "print (data.shape) # dimensions\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc24f7d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 314
    },
    "executionInfo": {
     "elapsed": 804,
     "status": "ok",
     "timestamp": 1618148290739,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "1fc24f7d",
    "outputId": "39f0ad7e-7b1a-4be5-ab75-fc83bec6cc67"
   },
   "outputs": [],
   "source": [
    "# Check dataset\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888e576e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 655,
     "status": "ok",
     "timestamp": 1618122565107,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "888e576e",
    "outputId": "7f317dee-48f0-4fea-df99-b3a704162f03"
   },
   "outputs": [],
   "source": [
    "# Check datatypes\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcdcac",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1133,
     "status": "ok",
     "timestamp": 1618148296534,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "dadcdcac",
    "outputId": "8e90534f-423e-4c00-b237-8cf8a13d4269"
   },
   "outputs": [],
   "source": [
    "# Check for null values (values contain no info and can be removed)\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31d3625",
   "metadata": {
    "id": "c31d3625"
   },
   "source": [
    "## 2.1 | Pre-visualization cleanup\n",
    "We can see that there are no NULL values, indicating clean data. However, the datatypes of certain columns need to be corrected for appropriate data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea820667",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "executionInfo": {
     "elapsed": 838,
     "status": "ok",
     "timestamp": 1618148301180,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "ea820667",
    "outputId": "c1e4feaa-62b7-4a60-b01d-65bf2c682104"
   },
   "outputs": [],
   "source": [
    "# Concatenate acquisition date and time into a single column\n",
    "data['period']=data['acq_date']+' '+data['acq_time'].astype(str) # this leaves a df with acq_date and acq_time still there\n",
    "# data.drop(columns=['acq_date','acq_time'], inplace=True)\n",
    "data['period']=pd.to_datetime(data['period'], format='%Y-%m-%d %H%M')\n",
    "data['acq_date']=pd.to_datetime(data['period'].dt.date, format='%Y-%m-%d')\n",
    "data['acq_time']=data['period'].dt.time\n",
    "data.sort_values(by=['period'], inplace=True) # observe that without this code, time does not flow correctly in the animation\n",
    "# data.set_index('period', inplace=True) # sets the index of the dataframe to be the period\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433b5097",
   "metadata": {
    "id": "433b5097"
   },
   "source": [
    "# 3 | Exploratory Data Analysis/Visualization\n",
    "In this section, we shall be doing some preliminary visualization of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77104861",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 761
    },
    "executionInfo": {
     "elapsed": 8105,
     "status": "ok",
     "timestamp": 1618148316545,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "77104861",
    "outputId": "0fd46a63-f5dd-48b7-c0e4-aa2c1ce0942c"
   },
   "outputs": [],
   "source": [
    "# Write animation to file\n",
    "fig1 = px.scatter_geo(data, \n",
    "                    lat='latitude', \n",
    "                    lon='longitude', \n",
    "                    scope='asia',\n",
    "                    center={'lat':2.2180,'lon':115.6628}, # centered to SEA\n",
    "                    color='confidence',\n",
    "                    animation_frame=data['period'].astype(str)) \n",
    "fig1.write_html('content/animation.html')\n",
    "# TODO: Fix animation to have constant legend \n",
    "\n",
    "# Display animation\n",
    "IFrame(src='content/animation.html', width=1080, height=720)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3d9506",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "executionInfo": {
     "elapsed": 5274,
     "status": "ok",
     "timestamp": 1618148327063,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "9b3d9506",
    "outputId": "21f80768-9304-4e84-d875-148e2703c3f8"
   },
   "outputs": [],
   "source": [
    "# FRP/confidence against time\n",
    "sns.relplot(x=\"period\", y=\"frp\", hue=\"confidence\", col=\"daynight\", data=data, height=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f851f132",
   "metadata": {
    "id": "f851f132"
   },
   "source": [
    "We note that there seems to be some data where the gaps between data is small. Hence, this necessitates the merging of time data into hourly frames to better analyze patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05a5dd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 722
    },
    "executionInfo": {
     "elapsed": 861,
     "status": "ok",
     "timestamp": 1618148332128,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "fa05a5dd",
    "outputId": "4eb14db2-de4c-474f-b51b-375de6592fcc"
   },
   "outputs": [],
   "source": [
    "# Processing data further to clean visualization\n",
    "\n",
    "# Generate new DFs with times rounded down to the nearest hour\n",
    "date_sorted = data\n",
    "date_sorted['period'] = date_sorted['period'].dt.floor('H')\n",
    "date_sorted_gb = date_sorted.groupby('acq_date') # returns a groupby object which can be called with below code\n",
    "\n",
    "# [date_sorted.get_group(x) for x in date_sorted_gb.groups] # this displays all the dataframes\n",
    "\n",
    "date_sorted.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0d6ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "executionInfo": {
     "elapsed": 3320,
     "status": "ok",
     "timestamp": 1618148340954,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "89e0d6ad",
    "outputId": "c1c34599-dcae-4042-c9da-f4ceb2dded65"
   },
   "outputs": [],
   "source": [
    "# Plot individual plots for every date\n",
    "import numpy as np\n",
    "pd.options.mode.chained_assignment = None  # ignore warnings\n",
    "# list_date = date_sorted['acq_date'].unique()\n",
    "\n",
    "# Formatting of plot\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "plt.xlabel('Time of Day (24H Format)')\n",
    "plt.ylabel('FRP')\n",
    "plt.xlim(0,2400)\n",
    "plt.xticks(np.linspace(0,2300,num=24), rotation=45)\n",
    "\n",
    "for x in date_sorted_gb.groups:\n",
    "    plt.title(str(x.date()))\n",
    "\n",
    "    # Data of plot\n",
    "    current = date_sorted_gb.get_group(x) # iterate through groups\n",
    "    current['acq_time'] = current['period'].dt.time # extract time\n",
    "    current['acq_time'] = current['acq_time'].apply(str) # convert to type string\n",
    "    current['acq_time'] = current['acq_time'].str.replace(':','').astype(int)/100 # convert to 24h format\n",
    "    plt.scatter(current['acq_time'],current['frp'])\n",
    "    plt.savefig('content/'+str(x.date())+'.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d462f310",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 387
    },
    "id": "d462f310",
    "outputId": "522408f1-66bc-4f09-c9c6-36f5a1418d8b"
   },
   "outputs": [],
   "source": [
    "filenames = date_sorted['acq_date'].dt.date.unique().astype(str)\n",
    "filenames = [(value+'.png') for value in filenames]\n",
    "\n",
    "images = []\n",
    "for filename in filenames:\n",
    "    images.append(imageio.imread('content/'+filename))\n",
    "imageio.mimwrite('content/dailyfrp.gif', images, format='gif', duration=1)\n",
    "\n",
    "for item in images:\n",
    "    display(Image(data=item))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028e879b",
   "metadata": {
    "id": "028e879b"
   },
   "source": [
    "Looking at the above visualizations, we can see that: \n",
    "- there tends to be a concentration of fire data within the same region\n",
    "- most fires are detected in the day\n",
    "- there seems to be a pattern in when the fires are detected within the same 7 day period\n",
    "\n",
    "We also notice a few points where the data is seemingly in the middle of the ocean.\n",
    "\n",
    "Consequently, this necessitates cleaning of data to remove unnecessary information. Feature selection is necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928d7f62",
   "metadata": {
    "id": "928d7f62"
   },
   "source": [
    "# 4 | Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc80d1",
   "metadata": {
    "id": "5ccc80d1"
   },
   "source": [
    "## 4.1 | Cleaning Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FsLFOFGRKj4w",
   "metadata": {
    "id": "FsLFOFGRKj4w"
   },
   "source": [
    "### 4.1.1 | Reformatting Features\n",
    "Certain columns need to be reformatted into their appropriate data type for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845ea24",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 808,
     "status": "ok",
     "timestamp": 1618148435510,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "1845ea24",
    "outputId": "9544558a-4033-40b9-ab17-71af01ce7b5b"
   },
   "outputs": [],
   "source": [
    "# Convert 'object' columns into appropriate dtype\n",
    "data['confidence'].astype('category')\n",
    "data['daynight'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11f2200",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "executionInfo": {
     "elapsed": 867,
     "status": "ok",
     "timestamp": 1618148438637,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "f11f2200",
    "outputId": "d2caca97-dc2d-40f6-aca6-14a61f26dd13"
   },
   "outputs": [],
   "source": [
    "# Convert time into integers\n",
    "data['acq_time'] = data['acq_time'].apply(str) # convert to type string\n",
    "data['acq_time'] = data['acq_time'].str.replace(':','').astype(int)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_jRhcVktK-If",
   "metadata": {
    "id": "_jRhcVktK-If"
   },
   "source": [
    "Certain columns of categorical data have multiple classifications in one column. To perform regression, categorical data need to be separated out into binary columns of dummy variables. To do so, we shall be using a tool known as OneHotEncoding (OHE).\n",
    "\n",
    "Source: [Dummy Variables](https://stats.idre.ucla.edu/spss/faq/coding-systems-for-categorical-variables-in-regression-analysis-2/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827f837b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "executionInfo": {
     "elapsed": 805,
     "status": "ok",
     "timestamp": 1618148441834,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "827f837b",
    "outputId": "e230ea1f-a3f8-41aa-b63c-46051f2b38d9"
   },
   "outputs": [],
   "source": [
    "# Resetting index for OHE preparation\n",
    "data.set_index('period', inplace=True)\n",
    "data.reset_index(inplace=True)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "icPNM1lKLnpb",
   "metadata": {
    "id": "icPNM1lKLnpb"
   },
   "source": [
    "### 4.1.2 | Supplementary Location Information (Mixed Dataset)\n",
    "As our dataset was limited in useful features, we wanted to add supplementary information. Using a geopandas package, we were able to get more information about the location of the fire using latitude and longitude. We were able to add information like the country, continent and estimated population of in the area, which could be useful to determine the intensity of the fire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84397066",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 621
    },
    "executionInfo": {
     "elapsed": 2156,
     "status": "ok",
     "timestamp": 1618150363981,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "84397066",
    "outputId": "55d1aed2-69ea-46a8-dc73-f58e377cd4dd"
   },
   "outputs": [],
   "source": [
    "# Converting latitude and longitude values to location values\n",
    "\n",
    "world = gpd.read_file(gpd.datasets.get_path('naturalearth_lowres'))\n",
    "gdf = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data['longitude'], data['latitude']), crs={'init': 'epsg:4326'})\n",
    "result = gpd.sjoin(gdf, world, how='left')\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eT5IQGz9A68",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "executionInfo": {
     "elapsed": 1033,
     "status": "ok",
     "timestamp": 1618150367862,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "0eT5IQGz9A68",
    "outputId": "7f3e9f27-5062-4d82-85f9-c320e5719a1a"
   },
   "outputs": [],
   "source": [
    "result.drop(['geometry','index_right','iso_a3','gdp_md_est'],axis=1,inplace=True)\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_06Vobj49A69",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1201,
     "status": "ok",
     "timestamp": 1618150373930,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "_06Vobj49A69",
    "outputId": "41833dca-f97a-42a3-afcf-5a4984bcbce6"
   },
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "result.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fB572Kga9A6-",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1521,
     "status": "ok",
     "timestamp": 1618150377681,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "fB572Kga9A6-",
    "outputId": "5690e676-2304-494a-83d6-f643186bcdc2"
   },
   "outputs": [],
   "source": [
    "# Above means there are null values present. \n",
    "# Solution: remove rows\n",
    "result.dropna(inplace=True)\n",
    "result.reset_index(drop=True) # reset index of dataframe to account for missing values\n",
    "result.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bPbgDCNa9A6_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 395
    },
    "executionInfo": {
     "elapsed": 970,
     "status": "ok",
     "timestamp": 1618151410645,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "bPbgDCNa9A6_",
    "outputId": "bfd91f58-ffdd-4803-d1bc-f14aa0062e8a"
   },
   "outputs": [],
   "source": [
    "# Visualize clean dataset\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e85c36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1618151413544,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "37e85c36",
    "outputId": "9b18baad-003f-4c7f-d7da-01d8a52752c0"
   },
   "outputs": [],
   "source": [
    "# One Hot Encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "ohe_confidence = pd.DataFrame(enc.fit_transform(result[['confidence']]).toarray())\n",
    "ohe_confidence.columns = enc.get_feature_names(['confidence'])\n",
    "#ohe_confidence.head()\n",
    "\n",
    "ohe_daynight = pd.DataFrame(enc.fit_transform(result[['daynight']]).toarray())\n",
    "ohe_daynight.columns = enc.get_feature_names(['daynight'])\n",
    "# ohe_daynight.head()\n",
    "\n",
    "ohe_continent = pd.DataFrame(enc.fit_transform(result[['continent']]).toarray())\n",
    "ohe_continent.columns = enc.get_feature_names(['continent'])\n",
    "# ohe_continent.head()\n",
    "\n",
    "ohe_name = pd.DataFrame(enc.fit_transform(result[['name']]).toarray())\n",
    "ohe_name.columns = enc.get_feature_names(['name'])\n",
    "# ohe_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "B0gUrz1gkAYT",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "executionInfo": {
     "elapsed": 912,
     "status": "ok",
     "timestamp": 1618151436493,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "B0gUrz1gkAYT",
    "outputId": "5f9e23fc-f6e7-453f-da5a-8dfd0556758c"
   },
   "outputs": [],
   "source": [
    "# Concatenate OHE variables with dataframe\n",
    "new_data = pd.concat([result,ohe_confidence,ohe_daynight,ohe_continent,ohe_name], axis=1)\n",
    "new_data.drop(['confidence', 'daynight','continent','name'], axis=1, inplace=True)\n",
    "new_data.dropna(inplace=True)\n",
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x94ibUymkNjK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1471,
     "status": "ok",
     "timestamp": 1618151440688,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "x94ibUymkNjK",
    "outputId": "8bb7832d-d34d-4edd-e599-8163b7bf22d9"
   },
   "outputs": [],
   "source": [
    "# Check datatypes of new dataframe\n",
    "new_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6NN3nsvEkZaY",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 788,
     "status": "ok",
     "timestamp": 1618151443558,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "6NN3nsvEkZaY",
    "outputId": "d613ba1a-d4bf-4dbb-a19b-7b1c8eac5dda"
   },
   "outputs": [],
   "source": [
    "# Standardization can only be done on numeric data - hence, columns not of int or float type should be removed.\n",
    "new_data.drop(['period','acq_date','latitude','longitude','version'],axis=1,inplace=True)\n",
    "new_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nTjswS_aVtHf",
   "metadata": {
    "id": "nTjswS_aVtHf"
   },
   "outputs": [],
   "source": [
    "#create a dataset to reference to\n",
    "data_mixed = new_data\n",
    "print(data_mixed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vaHvtKx0jwSz",
   "metadata": {
    "id": "vaHvtKx0jwSz"
   },
   "source": [
    "### 4.1.3 | Converting Continuous Features into Categorical Features (Binned Dataset)\n",
    "To make the model more accurate, we attempted to convert continuous features into categorical features by splitting the datapoints into 5 bins of equally spaced out ranges of continuous data. We will then use OHE to convert these bins into dummy variables. This is so that we can simplify and standardize the type of features which is fed to the models by converting all the features (X) to binary variables. We will be training our models using this preprocessed fully categorical dataset, as well as the mixed continous-categorical dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iBJt11x9kgJa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1251,
     "status": "ok",
     "timestamp": 1618152779037,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "iBJt11x9kgJa",
    "outputId": "ab51e709-ac31-43f7-c68e-d4c521b528b4"
   },
   "outputs": [],
   "source": [
    "# Generate labels for modified dataset\n",
    "mod_var = [[],[],[],[],[],[]]\n",
    "var = ['bright_ti4','scan','track','bright_ti5','pop_est']\n",
    "quantile = ['_20p','_40p','_60p','_80p','_100p']\n",
    "quartile = ['_25p','_50p','_75p','_100p']\n",
    "\n",
    "for x in range(len(var)):\n",
    "    mod_var[x] = [var[x] + value for value in quantile]\n",
    "    print (mod_var[x])\n",
    "mod_var[-1]=['acq_time'+value for value in quartile]\n",
    "print (mod_var[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o9XEF2Zpk0vx",
   "metadata": {
    "id": "o9XEF2Zpk0vx"
   },
   "outputs": [],
   "source": [
    "# Convert input to categorical input\n",
    "\n",
    "data_bright_ti4,data_bright_ti4_intervals=pd.qcut(new_data['bright_ti4'], 5, retbins=True,\n",
    "                        labels=mod_var[0])\n",
    "data_scan,data_scan_intervals=pd.qcut(new_data['scan'], 5, retbins=True,\n",
    "                        labels=mod_var[1])\n",
    "data_track,data_track_intervals=pd.qcut(new_data['track'], 5, retbins=True,\n",
    "                        labels=mod_var[2])\n",
    "data_bright_ti5,data_bright_ti5_intervals=pd.qcut(new_data['bright_ti5'], 5, retbins=True,\n",
    "                        labels=mod_var[3])\n",
    "data_pop_est,data_pop_est_intervals=pd.qcut(new_data['pop_est'], 5, retbins=True,\n",
    "                        labels=mod_var[4])\n",
    "data_acq_time,data_acq_time_intervals=pd.qcut(new_data['acq_time'], 4, retbins=True,\n",
    "                        labels=mod_var[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eHEm0Cy9kkDU",
   "metadata": {
    "id": "eHEm0Cy9kkDU"
   },
   "outputs": [],
   "source": [
    "#Concatenate all to a single DF\n",
    "cont_var = pd.concat([data_bright_ti4,data_scan,data_track,data_bright_ti5,data_pop_est,data_acq_time], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hy915ebcorNq",
   "metadata": {
    "id": "Hy915ebcorNq"
   },
   "outputs": [],
   "source": [
    "cont_var.dropna(inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SGMqdUl3ooMc",
   "metadata": {
    "id": "SGMqdUl3ooMc"
   },
   "outputs": [],
   "source": [
    "# One-Hot Encoding\n",
    "ohe_bright_ti4 = pd.DataFrame(enc.fit_transform(cont_var[['bright_ti4']]).toarray())\n",
    "ohe_bright_ti4.columns = enc.get_feature_names(['bright_ti4'])\n",
    "\n",
    "ohe_scan = pd.DataFrame(enc.fit_transform(cont_var[['scan']]).toarray())\n",
    "ohe_scan.columns = enc.get_feature_names(['scan'])\n",
    "\n",
    "ohe_track = pd.DataFrame(enc.fit_transform(cont_var[['track']]).toarray())\n",
    "ohe_track.columns = enc.get_feature_names(['track'])\n",
    "\n",
    "ohe_bright_ti5 = pd.DataFrame(enc.fit_transform(cont_var[['bright_ti5']]).toarray())\n",
    "ohe_bright_ti5.columns = enc.get_feature_names(['bright_ti5'])\n",
    "\n",
    "ohe_pop_est = pd.DataFrame(enc.fit_transform(cont_var[['pop_est']]).toarray())\n",
    "ohe_pop_est.columns = enc.get_feature_names(['pop_est'])\n",
    "\n",
    "ohe_acq_time = pd.DataFrame(enc.fit_transform(cont_var[['acq_time']]).toarray())\n",
    "ohe_acq_time.columns = enc.get_feature_names(['acq_time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2s0j76zco2Qc",
   "metadata": {
    "id": "2s0j76zco2Qc"
   },
   "outputs": [],
   "source": [
    "new_data.drop(['bright_ti4','scan','track','acq_time','bright_ti5','pop_est'],axis=1,inplace=True)\n",
    "new_data = pd.concat([ohe_bright_ti4,ohe_scan,ohe_track,ohe_bright_ti5,ohe_pop_est,ohe_acq_time,new_data],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I7ZdumDco3-g",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "executionInfo": {
     "elapsed": 1346,
     "status": "ok",
     "timestamp": 1618152797856,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "I7ZdumDco3-g",
    "outputId": "75b79622-b446-4a5b-f522-f52d88b50a9a"
   },
   "outputs": [],
   "source": [
    "new_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jRYSnoZkpv0a",
   "metadata": {
    "id": "jRYSnoZkpv0a"
   },
   "outputs": [],
   "source": [
    "new_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fVMNcZVzbG7O",
   "metadata": {
    "id": "fVMNcZVzbG7O"
   },
   "outputs": [],
   "source": [
    "#create a dataset to reference to\n",
    "data_binned = new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42979cc0",
   "metadata": {
    "id": "42979cc0"
   },
   "source": [
    "## 4.2 | Feature Selection\n",
    "\n",
    "With our two datasets, we shall do some feature selection to make our models run faster and smoother. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10c9a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binned Dataset\n",
    "Xmix = data_mixed.drop(['frp'], axis=1)\n",
    "ymix = pd.DataFrame(data_mixed['frp'])\n",
    "\n",
    "# Binned Dataset\n",
    "Xbin = data_binned.drop(['frp'], axis=1)\n",
    "ybin = pd.DataFrame(data_binned['frp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b0b1fa",
   "metadata": {},
   "source": [
    "### 4.2.1 | Covariance Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a26dc4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6a26dc4",
    "outputId": "140af981-30f4-40a6-fdc2-10d48bd23872"
   },
   "outputs": [],
   "source": [
    "##z-scoring standardization\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "def standardize(df):\n",
    "  # create a scaler object\n",
    "  std_scaler = StandardScaler()\n",
    "  # fit and transform the data\n",
    "  return pd.DataFrame(std_scaler.fit_transform(df), columns=df.columns)\n",
    "\n",
    "print(data_mixed.head())\n",
    "X_cleaned  = standardize(data_mixed)\n",
    "print(X_cleaned[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87490b47",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "id": "87490b47",
    "outputId": "5bd408cc-b2fa-4a15-ff06-317edf00c561"
   },
   "outputs": [],
   "source": [
    "Xdata = X_cleaned.copy()\n",
    "plt.figure(1)\n",
    "fig, ax = plt.subplots(figsize=(25,25)) \n",
    "sns.set()\n",
    "ax = sns.heatmap(Xdata.corr(method='pearson'),vmin=0,cmap=\"YlGnBu\",annot=True,ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s-3MU51Y9zgm",
   "metadata": {
    "id": "s-3MU51Y9zgm"
   },
   "source": [
    "### 4.2.2 | Random Forest Model\n",
    "\n",
    "Another model we utilised is the Random Forest model, which comprises of a user-determined number of decision trees. \n",
    "\n",
    "It works through the following steps:\n",
    "1. A random sample is selected from the given dataset.\n",
    "2. From this random sample, a decision tree is crafted. Assuming there are n random samples selected, there will be n decision trees. (The argument n_estimators allows the user to choose the value of n.)\n",
    "3. When the Random Forest model makes a prediction, each decision tree will then produce a predicted result. Voting will commence for all n decision trees.\n",
    "4. Once all n predicted results have been voted upon, the highest voted result is the final prediction.\n",
    "\n",
    "The model has proven to be both accurate and robust due to the number of decision trees involved; additionally, due to it taking the average of all predictions by the decision trees, it is not affected by overfitting.\n",
    "\n",
    "Sources: \n",
    "- [Random Forest in Python](https://towardsdatascience.com/random-forest-in-python-24d0893d51c0)\n",
    "- [Understanding Random Forests Classifiers in Python](https://www.datacamp.com/community/tutorials/random-forests-classifier-python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JDl6hyJcGdPd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "executionInfo": {
     "elapsed": 1423,
     "status": "error",
     "timestamp": 1618161971378,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "JDl6hyJcGdPd",
    "outputId": "f3f2c1d3-506b-49cf-ff1b-69f27c64ff1d"
   },
   "outputs": [],
   "source": [
    "##convert to numpy to use with randomforest\n",
    "\n",
    "X = Xmix\n",
    "y = ymix\n",
    "\n",
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 1000, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(X.values, y.values.ravel());\n",
    "\n",
    "# get importance\n",
    "importance = rf.feature_importances_\n",
    "# print(importance)\n",
    "# summarize feature importance\n",
    "important_features_dict = {}\n",
    "for i,v in enumerate(importance):\n",
    "    important_features_dict[X.columns[i]] = v\n",
    "\n",
    "sorted_values = sorted(important_features_dict.values(),reverse=True) # Sort the values\n",
    "sorted_dict = {}\n",
    "\n",
    "for i in sorted_values:\n",
    "    for k in important_features_dict.keys():\n",
    "        if important_features_dict[k] == i:\n",
    "            sorted_dict[k] = important_features_dict[k]\n",
    "            break\n",
    "         \n",
    "toplist = []\n",
    "for i in sorted_dict:\n",
    "    #print(i,sorted_dict[i])\n",
    "    toplist.append(i)\n",
    "\n",
    "first3vals = toplist[:3]\n",
    "print(\"The top 3 factors correlating to the Fire Radiative Power are %s, %s, %s\" % ( first3vals[0], first3vals[1] , first3vals[2] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E6ML0vR3Almd",
   "metadata": {
    "id": "E6ML0vR3Almd"
   },
   "source": [
    "# 5 | Data Analysis - Supervised Learning Models\n",
    "In this section, we shall be using our dataset to train three different models to attempt to predict a value of FRP (Fire Radiative Power). Fire Radiative Power is the detected thermal energy of the fire, which is indicative of the size and intensity of the wildfire.\n",
    "\n",
    "<p> From the feature selection, the relevant features that we are using to predict FRP values would be bright_ti4 and bright_ti5, and a high confidence of ths satelite in the conditions for measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "I0GVozpBEfEh",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1618124639861,
     "user": {
      "displayName": "qing 1",
      "photoUrl": "",
      "userId": "17349177959796089200"
     },
     "user_tz": -480
    },
    "id": "I0GVozpBEfEh",
    "outputId": "4988876a-8a33-48b4-ba40-9c6ba5787953"
   },
   "outputs": [],
   "source": [
    "data_xs_mixed = \n",
    "data_y_mixed = \n",
    "\n",
    "data_xs_binned = \n",
    "data_y_binned =\n",
    "\n",
    "print(data_xs[:5], data_xs.shape)\n",
    "print(data_y[:5], data_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nk3Y_9krqOaH",
   "metadata": {
    "id": "nk3Y_9krqOaH"
   },
   "outputs": [],
   "source": [
    "data_xs.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Fiz7tng6BnkZ",
   "metadata": {
    "id": "Fiz7tng6BnkZ"
   },
   "outputs": [],
   "source": [
    "#Visualization of our cleaned dataset\n",
    "def scatterplot_XY(x, y, x_label):\n",
    "  plt.scatter(x, y, s=60, c='r', marker='+', label='Class0')\n",
    "  #plt.xlabel(x.keys())\n",
    "  plt.ylabel('frp') \n",
    "  plt.xlabel(x_label)\n",
    "  plt.show()\n",
    "  plt.clf()\n",
    "\n",
    "for column in data_xs:\n",
    "  x= data_xs[column]\n",
    "  y = data_y\n",
    "  scatterplot_XY(x, y, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VWD_MKpB4lku",
   "metadata": {
    "id": "VWD_MKpB4lku"
   },
   "outputs": [],
   "source": [
    "#TO DO, or TO DELETE: categorical data visualization. \n",
    "#sns.relplot(kind ='bubble', x = data_xs['confidence_high'], y = data_y)\n",
    "##sns.scatterplot(data=data, x=\"gdpPercap\", y=\"lifeExp\", size=\"pop\", legend=False, sizes=data_xs)\n",
    "\n",
    "dfviolin = pd.DataFrame([data_xs['confidence_high'].values, data_y.values]).transpose()\n",
    "#dfviolin = pd.DataFrame(data=np.concatenate((data_xs['confidence_high'],y_pred),axis=1), columns=[\"Actual\",\"Predicted\"])\n",
    "\n",
    "print(dfviolin[:5])\n",
    "\n",
    "sns.violinplot(data = dfviolin, x='0', y = '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xy0zIINlSfRx",
   "metadata": {
    "id": "xy0zIINlSfRx"
   },
   "outputs": [],
   "source": [
    "# due to the high density of our points, we shall reduce their opacity and visualize them in terms of density\n",
    "#visualization\n",
    "def scatterplot_XY(x, y,x_label = \"x_variable\"):\n",
    "  plt.scatter(x, y, s=60, c='r', marker='+', alpha = 0.1, label='Class0')\n",
    "  #plt.xlabel(x.keys())\n",
    "  plt.ylabel('frp') \n",
    "  plt.xlabel(x_label)\n",
    "  plt.show()\n",
    "  plt.clf()\n",
    "\n",
    "\n",
    "for column in data_xs:\n",
    "  x= data_xs[column]\n",
    "  y = data_y\n",
    "  scatterplot_XY(x, y, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sd0egWTbAU1R",
   "metadata": {
    "id": "sd0egWTbAU1R"
   },
   "source": [
    "## 5.1 | Approach\n",
    "This is a regression problem, where we are attempting to predict a continous variable y = FRP, from 3 different features X = ['bright_ti4', 'bright_ti5', 'confidence_high']\n",
    "\n",
    "We will be applying 3 different regression models to attempt to quantify a relationship between the X and y variables. The models are: \n",
    "1.   Linear Regression\n",
    "2.   Polynomial Regression\n",
    "3.   Random Forest\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oK3JXGd_GpcT",
   "metadata": {
    "id": "oK3JXGd_GpcT"
   },
   "source": [
    "### 5.1.1 | Train-Validation-Test Split\n",
    "\n",
    "Before we work on the dataset, we are going to split the dataset into train sets and test sets. The train set will be used for creating and fitting our model parameters, while the test data is used to evaluate the accuracy and effectiveness of our model for predicting FRP values. \n",
    "\n",
    "For models that have hyperparameters to be calibrated, we shall also split the train set from above into train-validation sets and conduct cross-fold validation to calibrate the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dBc84uMBGaB8",
   "metadata": {
    "id": "dBc84uMBGaB8"
   },
   "outputs": [],
   "source": [
    "# data for the models here\n",
    "data_xs = data_xs\n",
    "data_y = data_y\n",
    "\n",
    "#train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_xs, data_y, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Shape of Train Dataset (X,y):\", X_train.shape, y_train.shape)\n",
    "print(\"Shape of Test Dataset (X,y):\", X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "epPm_elArSRK",
   "metadata": {
    "id": "epPm_elArSRK"
   },
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k37NU2bYc6Em",
   "metadata": {
    "id": "k37NU2bYc6Em"
   },
   "outputs": [],
   "source": [
    "#train-validation Cross Validation with 4 folds\n",
    "def crossvald(data_xs, data_y):\n",
    "  ### n_folds = 4\n",
    "  X_b1, X_b2, y_b1, y_b2 = train_test_split(data_xs, data_y, test_size=0.5, random_state=0)\n",
    "  X_1, X_2, y_1, y_2 = train_test_split(X_b1, y_b1, test_size=0.5, random_state=0)\n",
    "  X_3, X_4, y_3, y_4 = train_test_split(X_b2, y_b2, test_size=0.5, random_state=0)\n",
    "\n",
    "  cross_sets ={1:{'train_X': np.concatenate((X_2,X_3,X_4)),\n",
    "                  'train_y': np.concatenate((y_2,y_3,y_4)),\n",
    "                  'val_X': X_1,\n",
    "                  'val_y': y_1,\n",
    "                    },\n",
    "               2:{'train_X': np.concatenate((X_1,X_3,X_4)),\n",
    "                  'train_y': np.concatenate((y_1,y_3,y_4)),\n",
    "                  'val_X': X_2,\n",
    "                  'val_y': y_2,\n",
    "                    },\n",
    "               3:{'train_X': np.concatenate((X_2,X_1,X_4)),\n",
    "                  'train_y': np.concatenate((y_2,y_1,y_4)),\n",
    "                  'val_X': X_3,\n",
    "                  'val_y': y_3,\n",
    "                    },\n",
    "               4:{'train_X': np.concatenate((X_2,X_3,X_1)),\n",
    "                  'train_y': np.concatenate((y_2,y_3,y_1)),\n",
    "                  'val_X': X_4,\n",
    "                  'val_y': y_4,\n",
    "                    },\n",
    "               }\n",
    "  return cross_sets\n",
    "\n",
    "CValSets = crossvald(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "txNEV4UMeRSQ",
   "metadata": {
    "id": "txNEV4UMeRSQ"
   },
   "outputs": [],
   "source": [
    "print(\"Shape of the Train Set 1, (X,y):\", CValSets[1]['train_X'].shape, CValSets[1]['train_y'].shape)\n",
    "print(\"Shape of the Validation Set 1, (X,y):\",CValSets[1]['val_X'].shape, CValSets[1]['val_y'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NKwzQJmiDj4q",
   "metadata": {
    "id": "NKwzQJmiDj4q"
   },
   "source": [
    "### 5.1.2 | Accuracy Score Formulas\n",
    "\n",
    "To evaluate the accuracy of the regression model, we shall be using the following accuracy and loss formula across all the models.\n",
    "\n",
    "Percent Error: \n",
    "$$\n",
    "P.E. (y_{pred}, y_{actual}) \n",
    "= \\frac{error}{actual} \n",
    "= \\frac{|y_{pred}- y_{actual}| }{ | y_{pred}+y_{actual}|/2} \n",
    "$$\n",
    "\n",
    "While the actual value is  $y_{actual}$,  the mean of $y_{pred}$ and $y_{actual}$ is used to prevent division by zero errors. \n",
    "\n",
    "\\\n",
    "Accuracy:\n",
    "<br>\n",
    "\\begin{align}\n",
    " Acc. (y_{pred}, y_{actual}) \n",
    "& = 1- mean (P.E.) \\\\\n",
    "& = 1- \\frac{1}{n} *  \\sum\\frac{|y_{pred}- y_{actual}| }{ | y_{pred}+y_{actual}|/2} \n",
    "\\end{align}\n",
    "\n",
    "Loss:\n",
    "$$\n",
    "L(y_{pred}, y_{actual}) = \\frac{1}{n} *  \\sum (y_{pred} - y_{actual})^2\n",
    "$$\n",
    "\n",
    "We shall be using the sci-kit learn library for implementation of all the models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ASeFXsucOhx_",
   "metadata": {
    "id": "ASeFXsucOhx_"
   },
   "outputs": [],
   "source": [
    "def loss_mse(y_actual, y_pred):\n",
    "  n = len(y_pred)\n",
    "  total = (y_pred-y_actual).T.dot(y_pred-y_actual)\n",
    "  loss = 1/n* total\n",
    "  return loss\n",
    "\n",
    "def mape (y_actual, y_pred):\n",
    "  # Calculate mean absolute percentage error (MAPE) modified to account for zeros in the actual readings\n",
    "  mape = 100 * (abs(y_pred-y_actual) / ((abs(y_actual+y_pred))/2)).values\n",
    "  return mape\n",
    "\n",
    "def accuracy(y_actual, y_pred):\n",
    "  mape = 100 * (abs(y_pred-y_actual) / ((abs(y_actual+y_pred))/2)).values\n",
    "  # Calculate and display accuracy\n",
    "  accuracy = 100 - np.mean(mape)\n",
    "  return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CgPlSr5ORlhb",
   "metadata": {
    "id": "CgPlSr5ORlhb"
   },
   "outputs": [],
   "source": [
    "## Print Formats \n",
    "loss = loss_mse(y_test, y_pred)\n",
    "print(\"Loss Test Set:\", loss)\n",
    "\n",
    "y_pred_train = reg_train.predict(X_train)\n",
    "loss = loss_mse(y_train, y_pred_train)\n",
    "print(\"Loss Train set:\", loss)\n",
    "\n",
    "\n",
    "print('Accuracy:', round(accuracy(y_test, y_pred), 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vAYRv0lzRcCf",
   "metadata": {
    "id": "vAYRv0lzRcCf"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def sklearn_metrics(y_actual, y_pred):\n",
    "\n",
    "  print('Mean Absolute Error:', metrics.mean_absolute_error(y_actual, y_pred))\n",
    "  print('Mean Squared Error (Loss that we have defined):', metrics.mean_squared_error(y_actual, y_pred))\n",
    "  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_actual, y_pred)))\n",
    "  print('Accuracy (sklearn):', metrics.accuracy_score(y_actual, y_pred) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rj0KTw5GBUys",
   "metadata": {
    "id": "rj0KTw5GBUys"
   },
   "source": [
    "## 5.1 | Linear Regression Model\n",
    "Firstly, we shall be attempting to use a simple linear regression model to predict the continuous FRP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Afp_QSzGh5d",
   "metadata": {
    "id": "-Afp_QSzGh5d"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    " \n",
    "def linear_reg(X,y):\n",
    "  reg = LinearRegression().fit(X, y)\n",
    "\n",
    "  #R^2 value, which is between -1 and 1\n",
    "  print(\"Model Score (R^2):\", reg.score(X, y))\n",
    "\n",
    "  print(\"Coeffcients\", list(X.columns),\": \", reg.coef_)\n",
    "  print(\"Intercept:\", reg.intercept_)\n",
    "  return reg\n",
    "\n",
    "reg_train = linear_reg(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IALhd7DjN8w1",
   "metadata": {
    "id": "IALhd7DjN8w1"
   },
   "outputs": [],
   "source": [
    "y_pred = reg_train.predict(X_test)\n",
    "\n",
    "print(y_pred[:5], type(y_pred), y_pred.shape)\n",
    "\n",
    "df = pd.DataFrame(data=np.concatenate((y_test,y_pred),axis=1), columns=[\"Actual\",\"Predicted\"])\n",
    "\n",
    "print (df[:5])\n",
    "\n",
    "loss = loss_mse(y_pred, y_test)\n",
    "print(\"Loss Test Set\", loss)\n",
    "\n",
    "y_pred_train = reg_train.predict(X_train)\n",
    "loss = loss_mse(y_pred_train, y_train)\n",
    "print(\"Loss Train set:\", loss)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n",
    "print(accuracy(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "koCU9xEBMs5S",
   "metadata": {
    "id": "koCU9xEBMs5S"
   },
   "outputs": [],
   "source": [
    "#attempt at visualization\n",
    "def scatterplot_wmodel(x,y,f_pred):\n",
    "  plt.scatter(x, y, s=60, c='r', marker='+', label='Class0', alpha = 0.001)\n",
    "  #plt.xlabel(x.keys())\n",
    "  plt.ylabel('frp') \n",
    "\n",
    "  x_pred = np.linspace(min(x),max(x),100)\n",
    "  y_pred = f_pred(x_pred)\n",
    "  plt.plot(x_pred, y_pred)\n",
    "  plt.show()\n",
    "  plt.clf()\n",
    "\n",
    "i = 0\n",
    "for column in data_xs:\n",
    "  x= data_xs[column]\n",
    "  y = data_y  \n",
    "  x_pred = np.linspace(min(x),max(x),100)\n",
    "  y_pred = reg_train.coef_[0][i]*x_pred + reg_train.intercept_\n",
    "  plt.plot(x_pred,y_pred) \n",
    "  \n",
    "  plt.scatter(x, y, s=60, c='r', marker='+', label='Class0')\n",
    "  plt.show() \n",
    "  i += 1\n",
    "  plt.clf()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g09RkaRIYuC4",
   "metadata": {
    "id": "g09RkaRIYuC4"
   },
   "source": [
    "## 5.3 | Random Forest/Decision Tree\n",
    "Attempt to use random forest/Decision tree to model the variables\n",
    "\n",
    "implemented using this: https://towardsdatascience.com/random-forest-in-python-24d0893d51c0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HXwD86acYuxm",
   "metadata": {
    "id": "HXwD86acYuxm"
   },
   "outputs": [],
   "source": [
    "print(type(y_train))\n",
    "print(type(X_train))\n",
    "\n",
    "##convert to numpy to use with randomforest\n",
    "X_trainn = X_train.values\n",
    "y_trainn = y_train.values.reshape(-1,)\n",
    "\n",
    "print(type(y_trainn), y_trainn.shape)\n",
    "print(type(X_trainn), X_trainn.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o176IlgoXIQ1",
   "metadata": {
    "id": "o176IlgoXIQ1"
   },
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 20, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(X_trainn, y_trainn);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Ay69bO2Vp-Uy",
   "metadata": {
    "id": "Ay69bO2Vp-Uy"
   },
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rf.predict(X_test.values)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test.values.reshape(-1,))\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2))\n",
    "\n",
    "\n",
    "df = pd.DataFrame(data=np.concatenate((y_test,predictions.reshape(-1,1), errors.reshape(-1,1)),axis=1), columns=[\"Actual\",\"Predicted\", \"Absolute Error\"])\n",
    "\n",
    "print (df[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YdijtYd2qjoM",
   "metadata": {
    "id": "YdijtYd2qjoM"
   },
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE) modified to account for zeros in the actual readings\n",
    "mape = 100 * (errors / ((abs(y_test.values.reshape(-1,))+abs(predictions))/2))\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7OGmQuX1tQqG",
   "metadata": {
    "id": "7OGmQuX1tQqG"
   },
   "outputs": [],
   "source": [
    "feature_list = X_train.columns\n",
    "\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Import tools needed for visualization\n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "# Pull out one tree from the forest\n",
    "tree = rf.estimators_[5]\n",
    "# Export the image to a dot file\n",
    "export_graphviz(tree, out_file = 'tree.dot', feature_names = feature_list, rounded = True, precision = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4DJgcbEP0oW4",
   "metadata": {
    "id": "4DJgcbEP0oW4"
   },
   "outputs": [],
   "source": [
    "#run with caution Already generated in folder\n",
    "# Use dot file to create a graph\n",
    "# oh no this takes forever to excute - estimated time = 4mins\n",
    "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
    "# Write graph to a png file - estimated time >15mins\n",
    "graph.write_png('tree.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbUrg3l9ZL-H",
   "metadata": {
    "id": "cbUrg3l9ZL-H"
   },
   "source": [
    "##Neural Network Implementation\n",
    "Try a neural network I guess here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2zokdwrOtqET",
   "metadata": {
    "id": "2zokdwrOtqET"
   },
   "outputs": [],
   "source": [
    "#turn frp into categorical data? \n",
    "#I honestly have no clue how to do it\n",
    "#cos neural network is mostly for y is categorical data I feel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VnxUpGyiY4ds",
   "metadata": {
    "id": "VnxUpGyiY4ds"
   },
   "source": [
    "## Polynomial Model with Cross-Fold Validation\n",
    "Cross-Fold Validation is used to optimize for the d variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cZ9qEgCcOx3T",
   "metadata": {
    "id": "cZ9qEgCcOx3T"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "d = 4\n",
    "\n",
    "def poly_reg(X_train,y_train,d, X_test, y_test):\n",
    "  poly_reg = PolynomialFeatures(degree=d)\n",
    "  X_poly = poly_reg.fit_transform(X_train)\n",
    "\n",
    "  reg = LinearRegression().fit(X_poly, y_train)\n",
    "\n",
    "  X_poly_t = poly_reg.fit_transform(X_test)\n",
    "  #R^2 value, which is between -1 and 1\n",
    "  print(\"Training Loss of R^2:\", reg.score(X_poly, y_train))\n",
    "  print(\"Test Loss of R^2:\", reg.score(X_poly_t, y_test))\n",
    "  #print(\"Coeffcients [x1,x2] : \", reg.coef_)\n",
    "  #print(\"Intercept:\", reg.intercept_)\n",
    "  y_pred_t = reg.predict(X_poly_t)\n",
    "\n",
    "  #loss\n",
    "  mse = metrics.mean_squared_error(y_test, y_pred_t)\n",
    "  #score, this was the original metric i used \n",
    "  reg.score(X_poly_t, y_test)\n",
    "\n",
    "  return reg, X_poly, mse\n",
    "\n",
    "reg_train, X_poly, test_score = poly_reg(X_train, y_train, 4, X_test, y_test)\n",
    "\n",
    "#R^2 value, which is between -1 and 1\n",
    "#print(\"Model Loss of R^2:\", poly_reg.score(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g_pkcfEMOzmi",
   "metadata": {
    "id": "g_pkcfEMOzmi"
   },
   "outputs": [],
   "source": [
    "splitdata = crossvald(X_train, y_train)\n",
    "\n",
    "len(splitdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fGaVXEw6O4UL",
   "metadata": {
    "id": "fGaVXEw6O4UL"
   },
   "outputs": [],
   "source": [
    "##selection of ideal d\n",
    "def bestd (dim, splitdata):\n",
    "  meanscore = 0\n",
    "  bestscore = 0\n",
    "  best_set = None\n",
    "  best_model = None\n",
    "  for setno, setdata in splitdata.items():\n",
    "    #print(setno)\n",
    "    reg_train, X_poly, test_score = poly_reg(setdata['train_X'], setdata['train_y'], dim, setdata['val_X'], setdata['val_y'])\n",
    "    meanscore += test_score/len(splitdata)\n",
    "    if (test_score < bestscore):\n",
    "      bestscore = test_score\n",
    "      best_set = setno\n",
    "      best_model = reg_train\n",
    "\n",
    "  return meanscore, bestscore, best_model, best_set\n",
    "\n",
    "yd =[]\n",
    "for d in range(1,20):\n",
    "\n",
    "  meanscore, bestscore, best_model, best_set = bestd (d, splitdata)\n",
    "  yd.append({'meanscore':meanscore, 'bestscore':bestscore, 'best_model': best_model, 'best_set':best_set})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8Vg4NT-CO_5_",
   "metadata": {
    "id": "8Vg4NT-CO_5_"
   },
   "outputs": [],
   "source": [
    "# this is the plot when we cross validate for the ideal dimension d\n",
    "\n",
    "ydf = pd.DataFrame(data=yd)\n",
    "plt.scatter(range(1,20), ydf['meanscore'])\n",
    "plt.show\n",
    "\n",
    "\n",
    "best_dim = int(ydf[['meanscore']].idxmin()+1)\n",
    "best_reg = ydf.at[int(best_dim-1),'best_model']\n",
    "best_set = ydf.at[int(best_dim-1),'best_model']\n",
    "\n",
    "\n",
    "print(ydf)\n",
    "print(min(ydf['meanscore']),'best dimension:', best_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Qbwi3alRO_wE",
   "metadata": {
    "id": "Qbwi3alRO_wE"
   },
   "outputs": [],
   "source": [
    "polyreg = PolynomialFeatures(degree=best_dim)\n",
    "X_poly_t = polyreg.fit_transform(X_test)\n",
    "X_poly = polyreg.fit_transform(X_train)\n",
    "print(X_poly_t.shape)\n",
    "\n",
    "\n",
    "\n",
    "y_pred = best_reg.predict(X_poly_t)\n",
    "print(\"Coeffcients [x1,x2] : \", best_reg.coef_.shape)\n",
    "print(\"Intercept:\", best_reg.intercept_)\n",
    "\n",
    "\n",
    "print(y_pred[:5], type(y_pred), y_pred.shape)\n",
    "\n",
    "df = pd.DataFrame(data=np.concatenate((y_test,y_pred, abs(y_pred-y_test)),axis=1), columns=[\"Actual\",\"Predicted\", \"Error\"])\n",
    "\n",
    "print (df[:5])\n",
    "\n",
    "def loss_mse(y_pred, y_test):\n",
    "  n = len(y_pred)\n",
    "  total = (y_pred-y_test).T.dot(y_pred-y_test)\n",
    "  loss = 1/n* total\n",
    "  return loss\n",
    "\n",
    "loss = loss_mse(y_pred, y_test)\n",
    "print(\"Loss Test Set\", loss)\n",
    "\n",
    "y_pred_train = best_reg.predict(X_poly)\n",
    "loss = loss_mse(y_pred_train, y_train)\n",
    "print(\"Loss Train set:\", loss)\n",
    "\n",
    "from sklearn import metrics\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n",
    "\n",
    "\n",
    "# Calculate mean absolute percentage error (MAPE) modified to account for zeros in the actual readings\n",
    "mape = 100 * (abs(y_pred-y_test) / ((abs(y_test+y_pred))/2)).values\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_mblQ9yctzCK",
   "metadata": {
    "id": "_mblQ9yctzCK"
   },
   "source": [
    "# 6 | Models - Unsupervised Learning\n",
    "K-means clustering to identify hotspots from one day of datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VFaxN_TJy8q3",
   "metadata": {
    "id": "VFaxN_TJy8q3"
   },
   "outputs": [],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qOeW_aX5tyaH",
   "metadata": {
    "id": "qOeW_aX5tyaH"
   },
   "outputs": [],
   "source": [
    "#cleaning the data, the relevant data is just lat, long and frp, confidence\n",
    "#need to restructure the data such that it separates out the dates\n",
    "\n",
    "data_c = data[['latitude', 'longitude', 'frp', 'confidence']].groupby(data['acq_date'])\n",
    "\n",
    "data_w = list(data_c)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kWYiYmYd2RZl",
   "metadata": {
    "id": "kWYiYmYd2RZl"
   },
   "outputs": [],
   "source": [
    "fig1 = px.scatter_geo(data_w, \n",
    "                    lat='latitude', \n",
    "                    lon='longitude', \n",
    "                    scope='asia',\n",
    "                    center={'lat':2.2180,'lon':115.6628}, # centered to SEA\n",
    "                    color='frp',)\n",
    "fig1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "JoyU3lBm3u2C",
   "metadata": {
    "id": "JoyU3lBm3u2C"
   },
   "source": [
    "To conduct clustering, we need to find a suitable algorithm which is fast enough to deal with the number of data points we have.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FGmPwtgm3DHw",
   "metadata": {
    "id": "FGmPwtgm3DHw"
   },
   "outputs": [],
   "source": [
    "print(\"Number of data points we have=\", data_w.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94Kx1vF6CYtg",
   "metadata": {
    "id": "94Kx1vF6CYtg"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wrc64LlG4kcw",
   "metadata": {
    "id": "Wrc64LlG4kcw"
   },
   "outputs": [],
   "source": [
    "from scipy.cluster.vq import vq, kmeans, whiten\n",
    "from numpy import array\n",
    "import matplotlib.pyplot as plt\n",
    "features  = array([[ 1.9,2.3],\n",
    "                   [ 1.5,2.5],\n",
    "                   [ 0.8,0.6],\n",
    "                   [ 0.4,1.8],\n",
    "                   [ 0.1,0.1],\n",
    "                   [ 0.2,1.8],\n",
    "                   [ 2.0,0.5],\n",
    "                   [ 0.3,1.5],\n",
    "                   [ 1.0,1.0]])\n",
    "whitened = whiten(features)\n",
    "book = np.array((whitened[0],whitened[2]))\n",
    "kmeans(whitened,book)\n",
    "\n",
    "from numpy import random\n",
    "random.seed((1000,2000))\n",
    "codes = 3\n",
    "kmeans(whitened,codes)\n",
    "\n",
    "# Create 50 datapoints in two clusters a and b\n",
    "pts = 50\n",
    "a = np.random.multivariate_normal([0, 0], [[4, 1], [1, 4]], size=pts)\n",
    "b = np.random.multivariate_normal([30, 10],\n",
    "                                  [[10, 2], [2, 1]],\n",
    "                                  size=pts)\n",
    "features = np.concatenate((a, b))\n",
    "# Whiten data\n",
    "whitened = whiten(features)\n",
    "# Find 2 clusters in the data\n",
    "codebook, distortion = kmeans(whitened, 2)\n",
    "# Plot whitened data and cluster centers in red\n",
    "plt.scatter(whitened[:, 0], whitened[:, 1])\n",
    "plt.scatter(codebook[:, 0], codebook[:, 1], c='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "RO3mOB_ouhAD",
   "metadata": {
    "id": "RO3mOB_ouhAD"
   },
   "source": [
    "# 7 | I'm gonna try a different dataset because this dataset is so limited :(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ah-To9juz3g",
   "metadata": {
    "id": "6ah-To9juz3g"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "CKp2OMWnLL9u",
   "metadata": {
    "id": "CKp2OMWnLL9u"
   },
   "source": [
    "# Trash\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tIUMcByYGorh",
   "metadata": {
    "id": "tIUMcByYGorh"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_xs, data_y, test_size=0.4, random_state=0)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "\n",
    "##svm doesnt work because its meant for classification\n",
    "clf = svm.SVC(kernel='linear', C=1).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ys8bfo5-Y2Z-",
   "metadata": {
    "id": "ys8bfo5-Y2Z-"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "b32860ce",
    "6de5ee4a",
    "d76f3246",
    "433b5097",
    "NKwzQJmiDj4q",
    "cbUrg3l9ZL-H",
    "_mblQ9yctzCK",
    "RO3mOB_ouhAD",
    "CKp2OMWnLL9u"
   ],
   "name": "Project (5) (3).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "0a84f4e9513e11ca67866431d8df20f43fe616bff8acff9def79f28cd0bb3c91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
